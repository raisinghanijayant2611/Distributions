{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many assumptions that go behind a Linear Regression model. Following are some of them based on which the Maximum Likelihood or the target variable is estimated :\n",
    "- The dataset is a colection of iid Random Variables, i.e. each data point is independent of the previous or next data point, and every data point is drawn from some mechanism whose output is a distribution which is identically distributed (or same distribution sometimes) \n",
    "- The experiment is basically estimating the target variable based on the given set of inputs. Let the given set of inputs be denoted by <b>X</b> = [${x_{1},x_{2}......,x_{n}}$]. Let the target variable be denoted by <b>t</b> = [$t_{1},t_{2},....t_{n}$] .\n",
    "- It is also assumed that in practical, we don't get to see exactly the distribution of the target variable <b>t</b>. What we observe is an output with some noise/error <b>e</b>. Thus, the actualy target which we try to capture is summation of observed values and some error : \n",
    "\\begin{align*}\n",
    "t_{i} = y(x_{i},w_{i}) + e_{i}\n",
    "\\end{align*}\n",
    "where y(x,w) is the output we observe and try to estimate. \n",
    "- It is also assued that the error is normally distributed with mean 0 and precision ${\\beta^{-1}}$ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following these assumptions, we can write the likelihood function as: \n",
    "\n",
    "\\begin{align*}\n",
    "p(t|x,w,{\\beta^{-1}}) = {\\prod_{n=1}^NN(t|y(x,w),{\\beta^{-1}})}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, there is one more assumption that the conditional mean y is linear in terms of x,i.e. \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "y(x,w) = w^{T}{\\phi(x)}\n",
    "\\end{align*}\n",
    "\n",
    "where $w = (w_{1},w_{2},....,w_{n})^{T}$ and ${\\phi} = ({\\phi_{1}},{\\phi_{2}},.....{\\phi_{n}})^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us make that above equation as equation2\n",
    "\n",
    "Now, we also know that the normal distribution is defined by the formula: \n",
    "\n",
    "\\begin{align*}\n",
    "{\\frac{1}{\\sqrt{2{\\pi}}{\\sigma}}}{e^{\\frac{-(x-u)^{2}}{2{\\sigma^{2}}}}}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "putting this formula, conditional mean y(x,w) as u , target variable as x and precision, the equation becomes: \n",
    "\n",
    "\\begin{align*}\n",
    "p(t|x,w,{\\beta^{-1}}) = {\\prod_{n=1}^N{\\frac{1}{\\sqrt{2{\\pi}}{\\beta^{-1}}}}{e^{\\frac{-(t-y(x,w))^{2}}{2({\\beta^{-1}})^{2}}}}}\n",
    "\\end{align*}\n",
    "\n",
    "Taking log on both sides, the above equation becomes: \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "{\\ln({p(t|x,w,{\\beta^{-1}})})} = {\\sum_{n=1}^N{\\ln{\\frac{1}{\\sqrt{2{\\pi}}{\\beta^{-1}}}}{e^{\\frac{-(t-y(x,w))^{2}}{2({\\beta^{-1}})^{2}}}}}}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "= {\\sum_{n=1}^N({\\ln{\\frac{1}{\\sqrt{2{\\pi}}{\\beta^{-1}}}}} - (E_{D}{\\beta})})\n",
    "\\end{align*}\n",
    "\n",
    "where $E_{D} = {\\frac{1}{2}}{\\sum_{n=1}^N(t_{n} - y(x,w)}) = {\\frac{1}{2}}{\\sum_{n=1}^N(t_{n} - w^{T}{\\phi}(x_{n})})$  (from equation2) \n",
    "\n",
    "Thus, \n",
    "\n",
    "\\begin{align*}\n",
    "{\\ln({p(t|x,w,{\\beta^{-1}})})} = {\\frac{N}{2}}{\\ln{\\beta}} - {\\frac{N}{2}}{\\ln{2{\\pi}}} - {\\beta}{E_{D}}\n",
    "\\end{align*}\n",
    "\n",
    "Thus, maximizing the likelihood in terms of ${\\beta}$ or coefficients $w$ is equivalent to least square estimation.\n",
    "\n",
    "\n",
    "\n",
    "We further know that the gradient is maximization wrt to weights. Thus, by taking a derivative wrt to $w$ and equating it to 0 we can get the closed form equation of regression : \n",
    "\n",
    "\\begin{align*} \n",
    "=> derivative = {\\sum_{n=1}^Nt_{n}{\\phi}(x_{n})^{T} - w^{T}({\\sum_{n=1}^N{\\phi}(x_{n}){\\phi}(x_{n})^{T}}}) = 0\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*} \n",
    " => w^{T}({\\sum_{n=1}^N{\\phi}(x_{n}){\\phi}(x_{n})^{T}}) = {\\sum_{n=1}^Nt_{n}{\\phi}(x_{n})^{T}}\n",
    "\\end{align*}\n",
    "\n",
    "taking transpose on both sides and ignoring the summation\n",
    "\n",
    "\\begin{align*} \n",
    " => w({\\phi}(x_{n})^{T}{\\phi}(x_{n})) = {\\phi}(x_{n})t_{n}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*} \n",
    " => w= ({\\phi}(x_{n})^{T}{\\phi}(x_{n}))^{-1}{\\phi}(x_{n})t_{n}\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
